%! Author = stefano
%! Date = 22/11/2020

% Preamble
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{dirtree}
\usepackage{graphicx}
\graphicspath{ {../images/} }

\title{Pedestrian Intent Prediction}
\author{Stefano Zanatta, Pasquale Coscia, Lamberto Ballan}
\date{\today}

% Packages
\usepackage{amsmath}
\usepackage{hyperref}

% Document
\begin{document}
\maketitle
\section{Introduction}
    We implemented the work of Haziq Razali, Alexandre Alah:
    Pedestrian-Intention-Prediction\footnote{\url{https://github.com/lucyze/Pedestrian-Intention-Prediction}}.
    The original work consists in a Convolutional Neural Network - LSTM, with the goal to predict the intention
    of pedestrians on crossing the road.\\
    The model was originally trained on Lausanne Dataset (not public) and on JAAD Dataset\footnote{\url{http://data.nvision2.eecs.yorku.ca/JAAD_dataset/}}.
    The former was recorded by static cameras, the latter on moving cameras (placed in the cars).\\
    The goal of this project is to study the pipeline, reproduce the experiments on the JAAD dataset, then adapt and
    improve the model for the PIE dataset\footnote{\url{https://data.nvision2.eecs.yorku.ca/PIE_dataset/}}.

\section{Lausanne Dataset}
    %TODO general results on lausanne (by Haziq Razali)

\section{JAAD Dataset}
    The JAAD datasets consists of mp4 videos, each one associated to a rich description, frame to frame, of what is happening on a given moment:
    \begin{itemize}
        \item id of the pedestrian
        \item coordinates of the pedestrian on the relative frame
        \item standing, walking
        \item looking at the traffic
        \item hand gesture
        \item speeding up/slowing down
        \item crossing the road
        \item occlusion
    \end{itemize}
    For the target we only use the information about crossing the road.\\
    \includegraphics[width=\textwidth]{jaadscene}\\

\section{Pipeline}
\subsection*{Adapting the ground truth}
    The first step is to create the ground truth for the model by transforming the input into a csv.
    Originally, the Lausanne dataset required a step to locate the pedestrians (with an external object detector)
    and the (static) coordinates of the road.
    This is not longer required, because JAAD and PIE include the coordinates of the pedestrians and whether or not
    they are crossing the road.
    \begin{center}
    \begin{tabular}{ c c c c c c c c c c}
     frame & id & tlx & tly & width & height & walking & standing & looking & incrossing\\
     1,0 & 463 & 730 & 69 & 118 & 0 & 1 & 0 & 0 & 0\\
     1,0 & 463 & 730 & 69 & 118 & 0 & 1 & 0 & 0 & 1\\
    \end{tabular}
    \end{center}
\subsection*{Trajectories}
    The second Step consists in assigning, at each frame, the ``lifetime'' of the pedestrian in the video, and a ``cross'' column, indicating if they
    ever crossed the road in their lifetime.
    \begin{center}
    \begin{tabular}{ c c c}
     incrossing & lifetime & cross\\
     0 & 568 & 0\\
     0 & 568 & 0\\
    \end{tabular}
    \end{center}

\subsection*{Hungarian Tracker}
    Remove pedestrians with a lifetime below a threshold, and change pedestrians IDs to a numeric value.
    We had to change the algorithm that defined whether the pedestrian crossed or not the road, before it was designed for
    videos where the pedestrian always started from a (non-crossing) side of the screen and then, eventually, he could cross the road
    or not.
    Now the new algorithm does not make any assumption on the initial position, to adapt to the JAAD and PIE datasets, where
    pedestrians can alternate the crossing of the road with the non-crossing and start in a crossing position.
    For example, this is the first frame of the Jaad video n\Â°5. In this case we expect the model to predict ``will cross''
    for each pedestrian in the screen.
    \includegraphics[width=0.5\textwidth]{crossing0}\\
\subsection*{Crop Pedestrians}
    Crops each pedestrian into images, organized in sub-folders:
    \dirtree{%
        .1 dataset.
        .2 all.
        .3 crops.
        .4 name\_of\_the\_video.
        .5 id\_of\_the\_pedestrian.
        .6 id\_pedestrian\_png.
    }\\
    \includegraphics[width=0.4\textwidth]{crop}\\
    Then appends the path of the crop to each frame's annotation.
    \begin{center}
    \begin{tabular}{ c c c c c c c c c c c c c c}
     filename & folderpath\\
     00000001.png & crops\video\_0001\0000000000\\
     00000001.png & crops\video\_0001\0000000000\\
    \end{tabular}
    \end{center}
\subsection*{Save Scenes}
    Add the whole scenes to the dataset and adds their paths to the annotations.
    \dirtree{%
        .1 dataset.
        .2 all.
        .3 scenes.
        .4 name\_of\_the\_video.
        .5 id\_video\_png.
    }
    The new annotations columns are:
    \begin{center}
    \begin{tabular}{ c c }
     scene\_filename & scene\_folderpath \\
     00000001.png & scenes\video\_0001 \\
     00000001.png & scenes\video\_0001 \\
    \end{tabular}
    \end{center}

\subsection*{Split train test}
    In the last step the dataset is split into train and test:
    \dirtree{%
    .1 dataset.
    .2 train.
    }\\
    \dirtree{%
    .1 dataset.
    .2 test.
    }
\section{Model}
The model is composed of a VGG-16 pre trained CNN, a fully connected layer (feature embedder), a LSTM and
a linear classifier to predict if the pedestrian crossed the road.\\
\includegraphics[width=0.4\textwidth]{jaaderrorcrop}\\

\subsection*{VGG-16}
This is a pre-trained CNN that works as a feature extractor for each image.
This operation is executed for each image in the batch.
\subsection*{Feature Embedder}
Embed the CNN output into a 255 units vector.
Then performs dropout.

\subsection*{LSTM}
Process the sequence through the LSTM with ReLU activation function and dropout.


\subsection*{Linear Classifier}
Classify whether the pedestrian crossed the road or not.

\subsection*{Changes to the original model}
To improve the results, we added the entire scene as input for the prediction.
We added a new VGG-16 CNN, followed by a LSTM to create the sequence, then we merged the scene-LSTM output with the crops-LSTM
outputs and adapted the input size of the linear predictor.

\\
Another change was to add more actions as target for the linear predictor.
This makes the task harder, but should make the model more robust to overfitting.


\section{PIE Dataset}
\#todo

\section{PIE Pipeline}
\#todo


\section{Results}
    The model works best when there is a clear distinction between the pedestrians are not-crossing (for their initial lifetime)
    and crossing the road (at the end of their lifetime).
    It works poorly for large roads where the same pedestrians can move in and out the road repeatedly.\\
    The original experiments where done on datasets where the direction of movement of the pedestrians where already known.
    For the Jaad dataset, instead, the directions were unkown and the results were much worse.
    \includegraphics[width=\textwidth]{results_original}\\
    To get better results we had to remove the draw-trajectories min-lifetime thresholds (so a pedestrian is considered more often, even if he
    is crossing the road at frame 0 and leaves the screen shortly after).
    This change improved the results by increasing the dataset size.\\
    The following results show the base model trained on Jaad dataset and the model improved with the scenes input.\\
    (all results are based on test data).

    \begin{center}
    \begin{tabular}{| c c c c |}\hline
     Model & Precision & Recall & Accuracy\\
     Jaad & 0.750 & 0.558 & 0.6714285714285714 \\
     Jaad+Scenes & 0.742 & 0.535 & 0.7428571428571429\\
    \end{tabular}
    \end{center}

    The results improved by adding the scenes, but the train accuracy (0.977) is still much higher, showing some overfitting.
    We increased the model complexity to reduce overfitting, first by changing the VGG-16 CNN.

    \begin{center}
    \begin{tabular}{| c c c c |}\hline
    Model & Precision & Recall & Accuracy\\

    \end{tabular}
    \end{center}


\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
wide_resnet50_2

[INFO: train.py:  165]: Starting epoch 44
[INFO: train.py:  165]: Starting epoch 45
[INFO: train.py:  165]: Starting epoch 46
[INFO: train.py:  228]: Checking stats on val ...
[INFO: train.py:  230]: Checking stats on train ...
[INFO: train.py:  234]:   [val] d_accuracy: 0.514
[INFO: train.py:  234]:   [val] d_fn: 20.000
[INFO: train.py:  234]:   [val] d_fp: 14.000
[INFO: train.py:  234]:   [val] d_loss: 0.711
[INFO: train.py:  234]:   [val] d_precision: 0.622
[INFO: train.py:  234]:   [val] d_recall: 0.535
[INFO: train.py:  234]:   [val] d_tn: 13.000
[INFO: train.py:  234]:   [val] d_tp: 23.000
[INFO: train.py:  237]:   [train] d_accuracy: 0.601
[INFO: train.py:  237]:   [train] d_fn: 78.000
[INFO: train.py:  237]:   [train] d_fp: 63.000
[INFO: train.py:  237]:   [train] d_loss: 0.690
[INFO: train.py:  237]:   [train] d_precision: 0.677
[INFO: train.py:  237]:   [train] d_recall: 0.629
[INFO: train.py:  237]:   [train] d_tn: 80.000
[INFO: train.py:  237]:   [train] d_tp: 132.000
[INFO: train.py:  258]: Saving checkpoint to ./models/cnnlstm_jaad_standardcrop_with_model.pt
[INFO: train.py:  260]: Done.
[train] best accuracy at lowest loss  0.6260623229461756
[train] best accuracy at highest accuracy  0.6260623229461756
[val] best accuracy at lowest loss  0.6571428571428571
[val] best accuracy at highest accuracy  0.6571428571428571



googlenet


[INFO: train.py:  165]: Starting epoch 44
[INFO: train.py:  165]: Starting epoch 45
[INFO: train.py:  165]: Starting epoch 46
[INFO: train.py:  228]: Checking stats on val ...
[INFO: train.py:  230]: Checking stats on train ...
[INFO: train.py:  234]:   [val] d_accuracy: 0.586
[INFO: train.py:  234]:   [val] d_fn: 21.000
[INFO: train.py:  234]:   [val] d_fp: 8.000
[INFO: train.py:  234]:   [val] d_loss: 0.743
[INFO: train.py:  234]:   [val] d_precision: 0.733
[INFO: train.py:  234]:   [val] d_recall: 0.512
[INFO: train.py:  234]:   [val] d_tn: 19.000
[INFO: train.py:  234]:   [val] d_tp: 22.000
[INFO: train.py:  237]:   [train] d_accuracy: 0.663
[INFO: train.py:  237]:   [train] d_fn: 58.000
[INFO: train.py:  237]:   [train] d_fp: 61.000
[INFO: train.py:  237]:   [train] d_loss: 0.646
[INFO: train.py:  237]:   [train] d_precision: 0.712
[INFO: train.py:  237]:   [train] d_recall: 0.722
[INFO: train.py:  237]:   [train] d_tn: 83.000
[INFO: train.py:  237]:   [train] d_tp: 151.000
[INFO: train.py:  258]: Saving checkpoint to ./models/cnnlstm_jaad_standardcrop_with_model.pt
[INFO: train.py:  260]: Done.
[train] best accuracy at lowest loss  0.6883852691218131
[train] best accuracy at highest accuracy  0.6883852691218131
[val] best accuracy at lowest loss  0.6571428571428571
[val] best accuracy at highest accuracy  0.6857142857142857


